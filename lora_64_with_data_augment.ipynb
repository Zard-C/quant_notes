{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a580f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faeb4df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-large-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"google/vit-large-patch16-224-in21k\", use_fast=True)\n",
    "model = AutoModelForImageClassification.from_pretrained(\"google/vit-large-patch16-224-in21k\", use_auth_token=os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "050000d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "model.classifier = torch.nn.Linear(model.config.hidden_size, 1000)  # ImageNet æœ‰ 1000 ä¸ªç±»åˆ«\n",
    "model.config.num_labels = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec2a8b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1024])\n",
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "print(model.classifier.weight.shape)\n",
    "print(model.classifier.bias.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67ec5ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTForImageClassification(\n",
      "  (vit): ViTModel(\n",
      "    (embeddings): ViTEmbeddings(\n",
      "      (patch_embeddings): ViTPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): ViTEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x ViTLayer(\n",
      "          (attention): ViTAttention(\n",
      "            (attention): ViTSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): ViTSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ViTIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ViTOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=1024, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# print the model \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bb7b1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57fd1368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LoRA Model Summary ---\n",
      "trainable params: 28,311,552 || all params: 332,638,184 || trainable%: 8.5112\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType   \n",
    "\n",
    "target_modules = ['query', 'value', 'key', 'dense', 'intermediate.dense', 'output.dense']\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    # åˆ†ç±»ä»»åŠ¡ç±»å‹\n",
    "    task_type=TaskType.FEATURE_EXTRACTION, # æˆ– TaskType.IMAGE_CLASSIFICATION (å–å†³äº PEFT ç‰ˆæœ¬)\n",
    "    r=64,          # LoRA ç§© (Rank): å†³å®šå¼•å…¥çš„å‚æ•°é‡å’Œè¡¨è¾¾èƒ½åŠ›\n",
    "    lora_alpha=64, # ç¼©æ”¾å› å­ (é€šå¸¸è®¾ä¸º r çš„ä¸¤å€æˆ–ç›¸ç­‰)\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "model_peft = get_peft_model(model, lora_config)\n",
    "\n",
    "print(f\"--- LoRA Model Summary ---\")\n",
    "model_peft.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53a1babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model_peft.named_parameters():\n",
    "    if \"lora_\" not in name and \"classifier\" not in name:\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c935b101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForFeatureExtraction(\n",
      "  (base_model): LoraModel(\n",
      "    (model): ViTForImageClassification(\n",
      "      (vit): ViTModel(\n",
      "        (embeddings): ViTEmbeddings(\n",
      "          (patch_embeddings): ViTPatchEmbeddings(\n",
      "            (projection): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (encoder): ViTEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-23): 24 x ViTLayer(\n",
      "              (attention): ViTAttention(\n",
      "                (attention): ViTSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=64, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=64, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=64, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "                (output): ViTSelfOutput(\n",
      "                  (dense): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=64, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): ViTIntermediate(\n",
      "                (dense): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1024, out_features=64, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): ViTOutput(\n",
      "                (dense): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (layernorm_before): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (layernorm_after): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (classifier): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_peft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1edfc51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xformers unavailable â€” fallback to FlashAttention only.\n"
     ]
    }
   ],
   "source": [
    "model_peft.base_model.vit.encoder.gradient_checkpointing = True  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœå†…å­˜\n",
    "model_peft.base_model.vit.use_flash_attention = True      # å¯ç”¨ Flash Attention æå‡é€Ÿåº¦ï¼ˆå¦‚æœæ”¯æŒï¼‰\n",
    "\n",
    "try:\n",
    "    model.enable_xformers_memory_efficient_attention()\n",
    "except:\n",
    "    print(\"xformers unavailable â€” fallback to FlashAttention only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46fad0f",
   "metadata": {},
   "source": [
    "## Benchmark the base model performance on ImageNet-Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d690b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åŠ è½½è®­ç»ƒé›†...\n",
      "è®­ç»ƒé›†åŠ è½½æˆåŠŸ: 1281167 å¼ å›¾ç‰‡, 1000 ä¸ªç±»åˆ«ã€‚\n",
      "\n",
      "æ­£åœ¨åŠ è½½éªŒè¯é›†...\n",
      "éªŒè¯é›†åŠ è½½æˆåŠŸ: 50000 å¼ å›¾ç‰‡, 1000 ä¸ªç±»åˆ«ã€‚\n",
      "\n",
      "âœ… DataLoader å‡†å¤‡å°±ç»ª (Batch Size: 64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# --- 1. è·¯å¾„é…ç½® ---\n",
    "# å‡è®¾æ‚¨çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†è·¯å¾„å¦‚ä¸‹\n",
    "TRAIN_DIR = \"/root/autodl-tmp/imagenet/train\"\n",
    "VAL_DIR = \"/root/autodl-tmp/imagenet/val\"\n",
    "\n",
    "# --- 2. é¢„å¤„ç† (Transforms) ---\n",
    "# ImageNet æ ‡å‡†å½’ä¸€åŒ–å‚æ•°\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# æ¨¡å‹è¾“å…¥å°ºå¯¸ä¸å½’ä¸€åŒ–ä¿æŒä¸€è‡´\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandAugment(num_ops=2, magnitude=9),\n",
    "    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225)\n",
    "    ),\n",
    "    transforms.RandomErasing(p=0.25)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225)\n",
    "    ),\n",
    "])\n",
    "\n",
    "# --- 3. è¾…åŠ©å‡½æ•° (è§£å†³ .JPEG å¤§å°å†™é—®é¢˜) ---\n",
    "def is_valid_image(path):\n",
    "    \"\"\"\n",
    "    è‡ªå®šä¹‰å‡½æ•°ï¼Œç¡®ä¿åªåŠ è½½æœ‰æ•ˆå›¾ç‰‡æ–‡ä»¶ (åŒ…æ‹¬å¤§å†™ .JPEG)\n",
    "    \"\"\"\n",
    "    return path.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))\n",
    "\n",
    "# --- 4. åŠ è½½ Datasets (ä½¿ç”¨ ImageFolder) ---\n",
    "\n",
    "print(\"æ­£åœ¨åŠ è½½è®­ç»ƒé›†...\")\n",
    "try:\n",
    "    train_dataset = ImageFolder(\n",
    "        root=TRAIN_DIR,\n",
    "        transform=train_transform,\n",
    "        is_valid_file=is_valid_image # ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°å¤„ç†å¤§å°å†™\n",
    "    )\n",
    "    \n",
    "    print(f\"è®­ç»ƒé›†åŠ è½½æˆåŠŸ: {len(train_dataset)} å¼ å›¾ç‰‡, {len(train_dataset.classes)} ä¸ªç±»åˆ«ã€‚\")\n",
    "\n",
    "    print(\"\\næ­£åœ¨åŠ è½½éªŒè¯é›†...\")\n",
    "    val_dataset = ImageFolder(\n",
    "        root=VAL_DIR,\n",
    "        transform=val_transform,\n",
    "        is_valid_file=is_valid_image # åŒæ ·ä½¿ç”¨\n",
    "    )\n",
    "    \n",
    "    print(f\"éªŒè¯é›†åŠ è½½æˆåŠŸ: {len(val_dataset)} å¼ å›¾ç‰‡, {len(val_dataset.classes)} ä¸ªç±»åˆ«ã€‚\")\n",
    "\n",
    "    # --- 5. åˆ›å»º DataLoaders ---\n",
    "    BATCH_SIZE = 64 # æ ¹æ®æ‚¨çš„ GPU æ˜¾å­˜è°ƒæ•´ (ä¾‹å¦‚ 64, 128, 256)\n",
    "    NUM_WORKERS = 8  # æ ¹æ®æ‚¨çš„ CPU æ ¸å¿ƒæ•°è°ƒæ•´ (ä¾‹å¦‚ 4, 8, 16)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,      # è®­ç»ƒé›†éœ€è¦æ‰“ä¹±\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,   # é”å®šå†…å­˜ä»¥åŠ å¿« GPU ä¼ è¾“\n",
    "        drop_last=True     # ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´çš„æ‰¹æ¬¡\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,     # éªŒè¯é›†ä¸éœ€è¦æ‰“ä¹±\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\nâœ… DataLoader å‡†å¤‡å°±ç»ª (Batch Size: {BATCH_SIZE})\")\n",
    "\n",
    "    # (å¯é€‰) æ£€æŸ¥ç±»åˆ«æ˜ å°„æ˜¯å¦ä¸€è‡´\n",
    "    # print(train_dataset.class_to_idx)\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®ç›®å½•ã€‚è¯·æ£€æŸ¥è·¯å¾„é…ç½®: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ åŠ è½½æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af5924ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# 4ï¸âƒ£ éªŒè¯å‡½æ•°ï¼ˆTop-1 + Top-5ï¼‰\n",
    "# --------------------------------------\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct1 = 0\n",
    "    correct5 = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(pixel_values=images)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Top-1\n",
    "            _, pred = logits.max(dim=1)\n",
    "            correct1 += (pred == labels).sum().item()\n",
    "\n",
    "            # Top-5\n",
    "            top5 = logits.topk(5, dim=1).indices\n",
    "            correct5 += (top5 == labels.unsqueeze(1)).any(dim=1).sum().item()\n",
    "\n",
    "            total += labels.size(0)\n",
    "\n",
    "    top1 = 100 * correct1 / total\n",
    "    top5 = 100 * correct5 / total\n",
    "    return top1, top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87e58d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc = evaluate_model(model_peft, val_loader, device)\n",
    "# print(f\"\\n--- LoRA Model on ImageNet Validation Set ---\")\n",
    "# print(f\"Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fe787f",
   "metadata": {},
   "source": [
    "```shell\n",
    "--- LoRA Model on ImageNet Validation Set ---\n",
    "Accuracy: 0.072%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a80b84",
   "metadata": {},
   "source": [
    "## Train model to improve performance further with ImageNet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15ce924a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20018/20018 [59:12<00:00,  5.64batch/s, loss=2.5411, lr=9.14e-05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/20] | Loss: 3.6267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [01:56<00:00, 13.40batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Top-1: 77.17%, Top-5: 92.59%\n",
      "ğŸŸ¢ New Best Model! Top-1 = 77.17%, saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20018/20018 [58:55<00:00,  5.66batch/s, loss=1.6943, lr=6.89e-05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/20] | Loss: 2.1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [01:47<00:00, 14.58batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Top-1: 80.11%, Top-5: 93.33%\n",
      "ğŸŸ¢ New Best Model! Top-1 = 80.11%, saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20018/20018 [58:54<00:00,  5.66batch/s, loss=1.5115, lr=4.11e-05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3/20] | Loss: 1.7862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [01:47<00:00, 14.59batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Top-1: 81.32%, Top-5: 93.73%\n",
      "ğŸŸ¢ New Best Model! Top-1 = 81.32%, saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20018/20018 [58:52<00:00,  5.67batch/s, loss=1.5403, lr=1.86e-05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [4/20] | Loss: 1.6145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [01:47<00:00, 14.60batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Top-1: 81.82%, Top-5: 93.71%\n",
      "ğŸŸ¢ New Best Model! Top-1 = 81.82%, saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20018/20018 [58:55<00:00,  5.66batch/s, loss=1.6732, lr=1.00e-04] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5/20] | Loss: 1.5236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [01:48<00:00, 14.35batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Top-1: 82.05%, Top-5: 93.69%\n",
      "ğŸŸ¢ New Best Model! Top-1 = 82.05%, saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20018/20018 [59:07<00:00,  5.64batch/s, loss=1.7993, lr=9.78e-05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [6/20] | Loss: 1.6645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [01:48<00:00, 14.42batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Top-1: 80.87%, Top-5: 93.10%\n",
      "âš  No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20018/20018 [59:02<00:00,  5.65batch/s, loss=1.8548, lr=9.14e-05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [7/20] | Loss: 1.5743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [01:47<00:00, 14.56batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Top-1: 81.20%, Top-5: 93.01%\n",
      "âš  No improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20018/20018 [58:59<00:00,  5.66batch/s, loss=1.5942, lr=8.15e-05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8/20] | Loss: 1.4899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [01:47<00:00, 14.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Top-1: 81.49%, Top-5: 93.08%\n",
      "âš  No improvement for 3 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20018/20018 [59:00<00:00,  5.65batch/s, loss=1.8785, lr=6.89e-05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [9/20] | Loss: 1.4147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [01:47<00:00, 14.58batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Top-1: 81.74%, Top-5: 93.13%\n",
      "âš  No improvement for 4 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20018/20018 [59:02<00:00,  5.65batch/s, loss=1.2192, lr=5.50e-05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [10/20] | Loss: 1.3395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [01:47<00:00, 14.58batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Top-1: 82.29%, Top-5: 93.28%\n",
      "ğŸŸ¢ New Best Model! Top-1 = 82.29%, saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20018/20018 [58:58<00:00,  5.66batch/s, loss=1.5260, lr=4.11e-05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [11/20] | Loss: 1.2688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [01:47<00:00, 14.56batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Top-1: 82.30%, Top-5: 93.03%\n",
      "ğŸŸ¢ New Best Model! Top-1 = 82.30%, saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20018/20018 [58:58<00:00,  5.66batch/s, loss=0.9153, lr=2.85e-05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [12/20] | Loss: 1.2054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [01:47<00:00, 14.57batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Top-1: 82.55%, Top-5: 93.03%\n",
      "ğŸŸ¢ New Best Model! Top-1 = 82.55%, saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20018/20018 [59:00<00:00,  5.65batch/s, loss=0.9278, lr=1.86e-05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [13/20] | Loss: 1.1520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [01:47<00:00, 14.57batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Top-1: 82.66%, Top-5: 93.10%\n",
      "ğŸŸ¢ New Best Model! Top-1 = 82.66%, saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20018/20018 [59:00<00:00,  5.65batch/s, loss=1.1143, lr=1.22e-05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [14/20] | Loss: 1.1118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [01:47<00:00, 14.56batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Top-1: 82.63%, Top-5: 92.96%\n",
      "âš  No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20018/20018 [58:59<00:00,  5.66batch/s, loss=0.7987, lr=1.00e-04] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [15/20] | Loss: 1.0850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [01:47<00:00, 14.56batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Top-1: 82.61%, Top-5: 92.94%\n",
      "âš  No improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20018/20018 [58:58<00:00,  5.66batch/s, loss=1.3060, lr=9.94e-05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [16/20] | Loss: 1.3076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [01:47<00:00, 14.55batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Top-1: 81.46%, Top-5: 92.53%\n",
      "âš  No improvement for 3 epoch(s).\n",
      "â›” Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "# --------------------------------------\n",
    "# 1ï¸âƒ£ ç¯å¢ƒä¼˜åŒ–é€‰é¡¹\n",
    "# --------------------------------------\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = \"/root/.torch_inductor\"\n",
    "os.environ[\"TORCH_LOGS\"] = \"nothing\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# --------------------------------------\n",
    "# 2ï¸âƒ£ è®¾å¤‡å’Œæ—¥å¿—\n",
    "# --------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "writer = SummaryWriter(log_dir=\"/root/tf-logs\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 3ï¸âƒ£ æ¨¡å‹ä¸ä¼˜åŒ–å™¨\n",
    "# --------------------------------------\n",
    "model_peft = torch.compile(model_peft)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = AdamW(\n",
    "    filter(lambda p: p.requires_grad, model_peft.parameters()),\n",
    "    lr=1e-4,\n",
    "    betas=(0.85, 0.98),\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# --------------------------------------\n",
    "# 4ï¸âƒ£ æ­£ç¡®çš„ schedulerï¼ˆIteration çº§ CosineAnnealingWarmRestartsï¼‰\n",
    "# --------------------------------------\n",
    "iters_per_epoch = len(train_loader)\n",
    "\n",
    "scheduler = CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=iters_per_epoch * 5,   # 5 ä¸ª epoch ä¸ºä¸€ä¸ªå®Œæ•´çš„ cosine å‘¨æœŸ\n",
    "    T_mult=2,\n",
    "    eta_min=1e-5\n",
    ")\n",
    "\n",
    "# AMP æ··åˆç²¾åº¦\n",
    "use_bf16 = torch.cuda.is_bf16_supported()\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=not use_bf16)\n",
    "\n",
    "# --------------------------------------\n",
    "# 5ï¸âƒ£ è®­ç»ƒå¾ªç¯\n",
    "# --------------------------------------\n",
    "num_epochs = 20\n",
    "best_top1 = 0\n",
    "patience = 3\n",
    "num_bad_epochs = 0\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_peft.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "\n",
    "    for step, (images, labels) in enumerate(pbar):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # æ··åˆç²¾åº¦è®­ç»ƒ\n",
    "        with torch.amp.autocast(\n",
    "            device_type=\"cuda\",\n",
    "            dtype=torch.bfloat16 if use_bf16 else torch.float16\n",
    "        ):\n",
    "            outputs = model_peft(pixel_values=images)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_peft.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # -------- å…³é”®ï¼šiteration çº§ scheduler.step() --------\n",
    "        scheduler.step()\n",
    "\n",
    "        # -------- Logging --------\n",
    "        running_loss += loss.item()\n",
    "        writer.add_scalar(\"Train/Loss\", loss.item(), global_step)\n",
    "        writer.add_scalar(\"LR\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "        global_step += 1\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            loss=f\"{loss.item():.4f}\",\n",
    "            lr=f\"{optimizer.param_groups[0]['lr']:.2e}\"\n",
    "        )\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}] | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # ---------- éªŒè¯ ----------\n",
    "    top1, top5 = evaluate_model(model_peft, val_loader, device)\n",
    "    print(f\"Validation Top-1: {top1:.2f}%, Top-5: {top5:.2f}%\")\n",
    "\n",
    "    writer.add_scalar(\"Val/Top1\", top1, epoch)\n",
    "    writer.add_scalar(\"Val/Top5\", top5, epoch)\n",
    "\n",
    "    # ---------- Best Model ä¿å­˜ ----------\n",
    "    if top1 > best_top1:\n",
    "        best_top1 = top1\n",
    "        num_bad_epochs = 0\n",
    "        print(f\"ğŸŸ¢ New Best Model! Top-1 = {best_top1:.2f}%, saving...\")\n",
    "\n",
    "        # ä¿å­˜ LoRA adapter\n",
    "        model_peft.save_pretrained(\"vit_lora_best\")\n",
    "\n",
    "        # ä¿å­˜ classifier\n",
    "        torch.save(\n",
    "            model_peft.base_model.classifier.state_dict(),\n",
    "            \"vit_classifier_best.pt\"\n",
    "        )\n",
    "    else:\n",
    "        num_bad_epochs += 1\n",
    "        print(f\"âš  No improvement for {num_bad_epochs} epoch(s).\")\n",
    "\n",
    "    # ---------- Early Stop ----------\n",
    "    if epoch >= 15 and num_bad_epochs >= patience:\n",
    "        print(\"â›” Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da462aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_peft.save_pretrained(\"./vit_lora_imagenet_model_r64_final\")\n",
    "torch.save(model.classifier.state_dict(), \"./vit_lora_imagenet_model_r64_final/vit_classifier_head.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
