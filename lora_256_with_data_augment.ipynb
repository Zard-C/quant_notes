{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a580f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faeb4df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-large-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"google/vit-large-patch16-224-in21k\", use_fast=True)\n",
    "model = AutoModelForImageClassification.from_pretrained(\"google/vit-large-patch16-224-in21k\", use_auth_token=os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "050000d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "model.classifier = torch.nn.Linear(model.config.hidden_size, 1000)  # ImageNet æœ‰ 1000 ä¸ªç±»åˆ«\n",
    "model.config.num_labels = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec2a8b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1024])\n",
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "print(model.classifier.weight.shape)\n",
    "print(model.classifier.bias.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67ec5ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTForImageClassification(\n",
      "  (vit): ViTModel(\n",
      "    (embeddings): ViTEmbeddings(\n",
      "      (patch_embeddings): ViTPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): ViTEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x ViTLayer(\n",
      "          (attention): ViTAttention(\n",
      "            (attention): ViTSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): ViTSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ViTIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ViTOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=1024, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# print the model \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bb7b1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57fd1368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LoRA Model Summary ---\n",
      "trainable params: 113,246,208 || all params: 417,572,840 || trainable%: 27.1201\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType   \n",
    "\n",
    "target_modules = ['query', 'value', 'key', 'dense', 'intermediate.dense', 'output.dense']\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    # åˆ†ç±»ä»»åŠ¡ç±»å‹\n",
    "    task_type=TaskType.FEATURE_EXTRACTION, # æˆ– TaskType.IMAGE_CLASSIFICATION (å–å†³äº PEFT ç‰ˆæœ¬)\n",
    "    r=256,          # LoRA ç§© (Rank): å†³å®šå¼•å…¥çš„å‚æ•°é‡å’Œè¡¨è¾¾èƒ½åŠ›\n",
    "    lora_alpha=256, # ç¼©æ”¾å› å­ (é€šå¸¸è®¾ä¸º r çš„ä¸¤å€æˆ–ç›¸ç­‰)\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "model_peft = get_peft_model(model, lora_config)\n",
    "\n",
    "print(f\"--- LoRA Model Summary ---\")\n",
    "model_peft.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53a1babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model_peft.named_parameters():\n",
    "    if \"lora_\" not in name and \"classifier\" not in name:\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c935b101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForFeatureExtraction(\n",
      "  (base_model): LoraModel(\n",
      "    (model): ViTForImageClassification(\n",
      "      (vit): ViTModel(\n",
      "        (embeddings): ViTEmbeddings(\n",
      "          (patch_embeddings): ViTPatchEmbeddings(\n",
      "            (projection): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (encoder): ViTEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-23): 24 x ViTLayer(\n",
      "              (attention): ViTAttention(\n",
      "                (attention): ViTSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=256, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=256, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=256, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=256, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=256, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=256, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "                (output): ViTSelfOutput(\n",
      "                  (dense): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=256, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=256, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): ViTIntermediate(\n",
      "                (dense): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1024, out_features=256, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=256, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): ViTOutput(\n",
      "                (dense): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=256, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=256, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (layernorm_before): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (layernorm_after): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (classifier): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_peft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1edfc51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xformers unavailable â€” fallback to FlashAttention only.\n"
     ]
    }
   ],
   "source": [
    "model_peft.base_model.vit.encoder.gradient_checkpointing = True  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœå†…å­˜\n",
    "model_peft.base_model.vit.use_flash_attention = True      # å¯ç”¨ Flash Attention æå‡é€Ÿåº¦ï¼ˆå¦‚æœæ”¯æŒï¼‰\n",
    "\n",
    "try:\n",
    "    model.enable_xformers_memory_efficient_attention()\n",
    "except:\n",
    "    print(\"xformers unavailable â€” fallback to FlashAttention only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46fad0f",
   "metadata": {},
   "source": [
    "## Benchmark the base model performance on ImageNet-Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d690b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åŠ è½½è®­ç»ƒé›†...\n",
      "è®­ç»ƒé›†åŠ è½½æˆåŠŸ: 1281167 å¼ å›¾ç‰‡, 1000 ä¸ªç±»åˆ«ã€‚\n",
      "\n",
      "æ­£åœ¨åŠ è½½éªŒè¯é›†...\n",
      "éªŒè¯é›†åŠ è½½æˆåŠŸ: 50000 å¼ å›¾ç‰‡, 1000 ä¸ªç±»åˆ«ã€‚\n",
      "\n",
      "âœ… DataLoader å‡†å¤‡å°±ç»ª (Batch Size: 64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# --- 1. è·¯å¾„é…ç½® ---\n",
    "# å‡è®¾æ‚¨çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†è·¯å¾„å¦‚ä¸‹\n",
    "TRAIN_DIR = \"/root/autodl-tmp/imagenet/train\"\n",
    "VAL_DIR = \"/root/autodl-tmp/imagenet/val\"\n",
    "\n",
    "# --- 2. é¢„å¤„ç† (Transforms) ---\n",
    "# ImageNet æ ‡å‡†å½’ä¸€åŒ–å‚æ•°\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# æ¨¡å‹è¾“å…¥å°ºå¯¸ä¸å½’ä¸€åŒ–ä¿æŒä¸€è‡´\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandAugment(num_ops=2, magnitude=9),\n",
    "    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225)\n",
    "    ),\n",
    "    transforms.RandomErasing(p=0.25)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225)\n",
    "    ),\n",
    "])\n",
    "\n",
    "# --- 3. è¾…åŠ©å‡½æ•° (è§£å†³ .JPEG å¤§å°å†™é—®é¢˜) ---\n",
    "def is_valid_image(path):\n",
    "    \"\"\"\n",
    "    è‡ªå®šä¹‰å‡½æ•°ï¼Œç¡®ä¿åªåŠ è½½æœ‰æ•ˆå›¾ç‰‡æ–‡ä»¶ (åŒ…æ‹¬å¤§å†™ .JPEG)\n",
    "    \"\"\"\n",
    "    return path.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))\n",
    "\n",
    "# --- 4. åŠ è½½ Datasets (ä½¿ç”¨ ImageFolder) ---\n",
    "\n",
    "print(\"æ­£åœ¨åŠ è½½è®­ç»ƒé›†...\")\n",
    "try:\n",
    "    train_dataset = ImageFolder(\n",
    "        root=TRAIN_DIR,\n",
    "        transform=train_transform,\n",
    "        is_valid_file=is_valid_image # ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°å¤„ç†å¤§å°å†™\n",
    "    )\n",
    "    \n",
    "    print(f\"è®­ç»ƒé›†åŠ è½½æˆåŠŸ: {len(train_dataset)} å¼ å›¾ç‰‡, {len(train_dataset.classes)} ä¸ªç±»åˆ«ã€‚\")\n",
    "\n",
    "    print(\"\\næ­£åœ¨åŠ è½½éªŒè¯é›†...\")\n",
    "    val_dataset = ImageFolder(\n",
    "        root=VAL_DIR,\n",
    "        transform=val_transform,\n",
    "        is_valid_file=is_valid_image # åŒæ ·ä½¿ç”¨\n",
    "    )\n",
    "    \n",
    "    print(f\"éªŒè¯é›†åŠ è½½æˆåŠŸ: {len(val_dataset)} å¼ å›¾ç‰‡, {len(val_dataset.classes)} ä¸ªç±»åˆ«ã€‚\")\n",
    "\n",
    "    # --- 5. åˆ›å»º DataLoaders ---\n",
    "    BATCH_SIZE = 64 # æ ¹æ®æ‚¨çš„ GPU æ˜¾å­˜è°ƒæ•´ (ä¾‹å¦‚ 64, 128, 256)\n",
    "    NUM_WORKERS = 8  # æ ¹æ®æ‚¨çš„ CPU æ ¸å¿ƒæ•°è°ƒæ•´ (ä¾‹å¦‚ 4, 8, 16)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,      # è®­ç»ƒé›†éœ€è¦æ‰“ä¹±\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,   # é”å®šå†…å­˜ä»¥åŠ å¿« GPU ä¼ è¾“\n",
    "        drop_last=True     # ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´çš„æ‰¹æ¬¡\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,     # éªŒè¯é›†ä¸éœ€è¦æ‰“ä¹±\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\nâœ… DataLoader å‡†å¤‡å°±ç»ª (Batch Size: {BATCH_SIZE})\")\n",
    "\n",
    "    # (å¯é€‰) æ£€æŸ¥ç±»åˆ«æ˜ å°„æ˜¯å¦ä¸€è‡´\n",
    "    # print(train_dataset.class_to_idx)\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®ç›®å½•ã€‚è¯·æ£€æŸ¥è·¯å¾„é…ç½®: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ åŠ è½½æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af5924ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# 4ï¸âƒ£ éªŒè¯å‡½æ•°ï¼ˆTop-1 + Top-5ï¼‰\n",
    "# --------------------------------------\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct1 = 0\n",
    "    correct5 = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(pixel_values=images)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Top-1\n",
    "            _, pred = logits.max(dim=1)\n",
    "            correct1 += (pred == labels).sum().item()\n",
    "\n",
    "            # Top-5\n",
    "            top5 = logits.topk(5, dim=1).indices\n",
    "            correct5 += (top5 == labels.unsqueeze(1)).any(dim=1).sum().item()\n",
    "\n",
    "            total += labels.size(0)\n",
    "\n",
    "    top1 = 100 * correct1 / total\n",
    "    top5 = 100 * correct5 / total\n",
    "    return top1, top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87e58d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc = evaluate_model(model_peft, val_loader, device)\n",
    "# print(f\"\\n--- LoRA Model on ImageNet Validation Set ---\")\n",
    "# print(f\"Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fe787f",
   "metadata": {},
   "source": [
    "```shell\n",
    "--- LoRA Model on ImageNet Validation Set ---\n",
    "Accuracy: 0.072%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a80b84",
   "metadata": {},
   "source": [
    "## Train model to improve performance further with ImageNet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ce924a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     26\u001b[39m optimizer = AdamW(\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m p: p.requires_grad, model_peft.parameters()),\n\u001b[32m     28\u001b[39m     lr=base_lr,\n\u001b[32m     29\u001b[39m     betas=(\u001b[32m0.85\u001b[39m, \u001b[32m0.98\u001b[39m),\n\u001b[32m     30\u001b[39m     weight_decay=\u001b[32m0.01\u001b[39m\n\u001b[32m     31\u001b[39m )\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# ====================================================\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# 2ï¸âƒ£ EMA æ¨¡å—\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# ====================================================\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# åˆ›å»ºç‹¬ç«‹ EMA æƒé‡å‰¯æœ¬\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m ema_model = \u001b[43mcopy\u001b[49m.deepcopy(model_peft).eval()\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m ema_model.parameters():\n\u001b[32m     39\u001b[39m     p.requires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'copy' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "import copy\n",
    "\n",
    "# ====================================================\n",
    "# 1ï¸âƒ£ è®­ç»ƒé…ç½®\n",
    "# ====================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "writer = SummaryWriter(log_dir=\"/root/tf-logs\")\n",
    "\n",
    "num_epochs = 10\n",
    "warmup_epochs = 1\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "warmup_steps = len(train_loader) * warmup_epochs\n",
    "final_lr = 1e-6   # ğŸ‘ˆ ä¸é™åˆ° 0ï¼Œæ›´ç¨³å®š\n",
    "base_lr = 1e-4\n",
    "ema_decay = 0.9997\n",
    "\n",
    "# compile (ä¿æŒå…¨å±€å¯ä¼˜åŒ–ä¸”ä¸æŸå PEFT)\n",
    "model_peft = torch.compile(model_peft, mode=\"max-autotune\", fullgraph=False)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = AdamW(\n",
    "    filter(lambda p: p.requires_grad, model_peft.parameters()),\n",
    "    lr=base_lr,\n",
    "    betas=(0.85, 0.98),\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# ====================================================\n",
    "# 2ï¸âƒ£ EMA æ¨¡å—\n",
    "# ====================================================\n",
    "# åˆ›å»ºç‹¬ç«‹ EMA æƒé‡å‰¯æœ¬\n",
    "ema_model = copy.deepcopy(model_peft).eval()\n",
    "for p in ema_model.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "def ema_update(model, ema_model, decay):\n",
    "    with torch.no_grad():\n",
    "        model_params = dict(model.named_parameters())\n",
    "        ema_params = dict(ema_model.named_parameters())\n",
    "\n",
    "        for k in model_params.keys():\n",
    "            if model_params[k].dtype.is_floating_point:\n",
    "                ema_params[k].data.mul_(decay).add_(model_params[k].data, alpha=1-decay)\n",
    "\n",
    "# ====================================================\n",
    "# 3ï¸âƒ£ Warmup + Linear Decay å­¦ä¹ ç‡å‡½æ•°\n",
    "# ====================================================\n",
    "def get_lr(step):\n",
    "    if step < warmup_steps:\n",
    "        return base_lr * (step / warmup_steps)  # linear warmup\n",
    "    else:\n",
    "        decay_ratio = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "        return final_lr + (base_lr - final_lr) * (1 - decay_ratio)\n",
    "\n",
    "# ====================================================\n",
    "# 4ï¸âƒ£ è®­ç»ƒå¾ªç¯\n",
    "# ====================================================\n",
    "use_bf16 = torch.cuda.is_bf16_supported()\n",
    "scaler = torch.amp.GradScaler(enabled=not use_bf16)\n",
    "\n",
    "best_top1 = 0\n",
    "patience = 3\n",
    "bad_epochs = 0\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_peft.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # æ›´æ–°å­¦ä¹ ç‡\n",
    "        lr = get_lr(global_step)\n",
    "        for pg in optimizer.param_groups:\n",
    "            pg[\"lr\"] = lr\n",
    "\n",
    "        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16 if use_bf16 else torch.float16):\n",
    "            outputs = model_peft(pixel_values=images)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_peft.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # EMA æ›´æ–°\n",
    "        ema_update(model_peft, ema_model, ema_decay)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        writer.add_scalar(\"Train/Loss\", loss.item(), global_step)\n",
    "        writer.add_scalar(\"LR\", lr, global_step)\n",
    "\n",
    "        global_step += 1\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\", lr=f\"{lr:.2e}\")\n",
    "\n",
    "    # =============== éªŒè¯ï¼šä½¿ç”¨ EMA æ¨¡å‹æ›´ç¨³ ===============\n",
    "    top1, top5 = evaluate_model(ema_model, val_loader, device)\n",
    "    print(f\"Validation Top1={top1:.2f}% | Top5={top5:.2f}%\")\n",
    "\n",
    "    if top1 > best_top1:\n",
    "        best_top1 = top1\n",
    "        bad_epochs = 0\n",
    "        print(f\"ğŸŸ¢ Best updated â†’ {best_top1:.2f}%, saving weights\")\n",
    "\n",
    "        # ä¿å­˜ LoRA\n",
    "        model_peft.save_pretrained(\"vit_lora_r256_ema_best\")\n",
    "\n",
    "        # ä¿å­˜åˆ†ç±»å¤´\n",
    "        torch.save(\n",
    "            model_peft.base_model.classifier.state_dict(),\n",
    "            \"vit_classifier_r256_ema_best.pt\"\n",
    "        )\n",
    "\n",
    "        # ä¿å­˜ EMA å®Œæ•´æ¨ç†æ¨¡å‹ï¼ˆå¯é€‰ï¼‰\n",
    "        torch.save(ema_model.state_dict(), \"vit_full_ema_state.pt\")\n",
    "\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "        print(f\"âš  {bad_epochs} bad epoch(s)\")\n",
    "\n",
    "    if bad_epochs >= patience:\n",
    "        print(\"â›” Early stopping\")\n",
    "        break\n",
    "\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
